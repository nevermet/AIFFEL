{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling News Articles and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤러를 만들기 전 필요한 도구들을 임포트합니다.\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\n",
    "def make_urllist(page_num, code, date): \n",
    "  urllist= []\n",
    "  for i in range(1, page_num + 1):\n",
    "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)   \n",
    "    news = requests.get(url)\n",
    "\n",
    "    # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "    soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "    # CASE 1\n",
    "    news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "    # CASE 2\n",
    "    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "    for line in news_list:\n",
    "        urllist.append(line.a.get('href'))\n",
    "  return urllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "#- 데이터프레임을 생성하는 함수입니다.\n",
    "def make_data(urllist, code):\n",
    "  text_list = []\n",
    "  for url in urllist:\n",
    "    article = Article(url, language='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text_list.append(article.text)\n",
    "\n",
    "  #- 데이터프레임의 'news' 키 아래 파싱한 텍스트를 밸류로 붙여줍니다.\n",
    "  df = pd.DataFrame({'news': text_list})\n",
    "\n",
    "  #- 데이터프레임의 'code' 키 아래 한글 카테고리명을 붙여줍니다.\n",
    "  df['code'] = idx2word[str(code)]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_list = [102, 103, 105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_total_data(page_num, code_list, date):\n",
    "  df = None\n",
    "\n",
    "  for code in code_list:\n",
    "    url_list = make_urllist(page_num, code, date)\n",
    "    df_temp = make_data(url_list, code)\n",
    "    print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
    "\n",
    "    if df is not None:\n",
    "      df = pd.concat([df, df_temp])\n",
    "    else:\n",
    "      df = df_temp\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>파주시청. 사진제공=파주시 파주시청. 사진제공=파주시\\n\\n[파주=파이낸셜뉴스 강근...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>동영상 뉴스\\n\\n이천 물류창고 화재 발화지점으로 지목된 지하 2층에서 산소절단기의...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>황범순 의정부시 부시장 을지대학교 의정부캠퍼스 및 부속병원 공사현장 안전점검. 사진...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>귀갓길 여성을 쫓아가 성범죄를 시도한 20대 남성이 구속됐습니다.서울 강남경찰서는 ...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(서울=연합뉴스) 대한약사회가 6일부터 코로나바이러스 감염증 대응 체계를 '사회적 ...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news code\n",
       "0  파주시청. 사진제공=파주시 파주시청. 사진제공=파주시\\n\\n[파주=파이낸셜뉴스 강근...   사회\n",
       "1  동영상 뉴스\\n\\n이천 물류창고 화재 발화지점으로 지목된 지하 2층에서 산소절단기의...   사회\n",
       "2  황범순 의정부시 부시장 을지대학교 의정부캠퍼스 및 부속병원 공사현장 안전점검. 사진...   사회\n",
       "3  귀갓길 여성을 쫓아가 성범죄를 시도한 20대 남성이 구속됐습니다.서울 강남경찰서는 ...   사회\n",
       "4  (서울=연합뉴스) 대한약사회가 6일부터 코로나바이러스 감염증 대응 체계를 '사회적 ...   사회"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "df = pd.read_table(csv_path, sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글 아닌 것 제외\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 기사 제거\n",
    "df.drop_duplicates(subset=['news'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 패키지 라이브러리\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 정의\n",
    "\n",
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하',\n",
    "             '한','다','과','때문','할','수','무단','따른','및','금지',\n",
    "             '전재','경향신문','기자','는데','가','등','들','파이낸셜','저작',\n",
    "             '등','뉴스']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수입니다.\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "    temp_data = []\n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer.morphs(sentence) \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리\n",
    "text_data = preprocessing(df['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.74      0.81       236\n",
      "          사회       0.79      0.91      0.85       422\n",
      "       생활/문화       0.81      0.76      0.79       341\n",
      "\n",
      "    accuracy                           0.82       999\n",
      "   macro avg       0.83      0.80      0.81       999\n",
      "weighted avg       0.82      0.82      0.82       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1. 형태소 분석기 변경해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.75      0.82       236\n",
      "          사회       0.80      0.91      0.85       422\n",
      "       생활/문화       0.81      0.77      0.79       341\n",
      "\n",
      "    accuracy                           0.82       999\n",
      "   macro avg       0.84      0.81      0.82       999\n",
      "weighted avg       0.83      0.82      0.82       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#형태소 패키지 변경 - Kkma\n",
    "from konlpy.tag import Kkma\n",
    "tokenizer = Kkma()\n",
    "\n",
    "text_data = preprocessing(df['news'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.93      0.85      0.89       355\n",
      "          사회       0.80      0.91      0.85       500\n",
      "       생활/문화       0.86      0.79      0.82       426\n",
      "\n",
      "    accuracy                           0.85      1281\n",
      "   macro avg       0.87      0.85      0.86      1281\n",
      "weighted avg       0.86      0.85      0.85      1281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#형태소 패키지 변경 - Hannanum\n",
    "from konlpy.tag import Hannanum\n",
    "tokenizer = Hannanum()\n",
    "\n",
    "text_data = preprocessing(df['news'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.93      0.84      0.88       355\n",
      "          사회       0.79      0.91      0.85       500\n",
      "       생활/문화       0.86      0.78      0.82       426\n",
      "\n",
      "    accuracy                           0.85      1281\n",
      "   macro avg       0.86      0.84      0.85      1281\n",
      "weighted avg       0.85      0.85      0.85      1281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#형태소 패키지 변경 - Komoran\n",
    "from konlpy.tag import Komoran\n",
    "tokenizer = Komoran()\n",
    "\n",
    "text_data = preprocessing(df['news'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.93      0.84      0.88       355\n",
      "          사회       0.79      0.92      0.85       500\n",
      "       생활/문화       0.87      0.78      0.82       426\n",
      "\n",
      "    accuracy                           0.85      1281\n",
      "   macro avg       0.86      0.84      0.85      1281\n",
      "weighted avg       0.86      0.85      0.85      1281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#형태소 패키지 변경 - Okt\n",
    "from konlpy.tag import Okt\n",
    "tokenizer = Okt()\n",
    "\n",
    "text_data = preprocessing(df['news'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2. 불용어 추가해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.74      0.81       236\n",
      "          사회       0.79      0.90      0.84       422\n",
      "       생활/문화       0.80      0.76      0.78       341\n",
      "\n",
      "    accuracy                           0.81       999\n",
      "   macro avg       0.83      0.80      0.81       999\n",
      "weighted avg       0.82      0.81      0.81       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#불용어 추가해 보기\n",
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하',\n",
    "             '한','다','과','때문','할','수','무단','따른','및','금지',\n",
    "             '전재','경향신문','기자','는데','가','등','들','파이낸셜','저작',\n",
    "             '등','뉴스', '헤럴드', '한국일보', '이데일리', '연합뉴스', '아시아경제', '뉴시스', '한국경제', '더팩트',\n",
    "             '서울신문', '서울경제', '노컷뉴스', '뉴스데스크', '한국일보닷컴', '재배포', '리포트', '앵커', '프레시안', '민중의소리',\n",
    "             '여성신문', '한겨레신문', '매일신문', '국민일보', '중앙일보', '데일리안', '강원일보', '오마이뉴스', '디지털타임즈', '온케이웨더' ]\n",
    "\n",
    "\n",
    "tokenizer = Mecab()\n",
    "\n",
    "text_data = preprocessing(df['news'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3. 다른 날짜 데이터 추가해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102번 코드에 대한 데이터를 만들었습니다.\n",
      "103번 코드에 대한 데이터를 만들었습니다.\n",
      "105번 코드에 대한 데이터를 만들었습니다.\n",
      "102번 코드에 대한 데이터를 만들었습니다.\n",
      "103번 코드에 대한 데이터를 만들었습니다.\n",
      "105번 코드에 대한 데이터를 만들었습니다.\n"
     ]
    }
   ],
   "source": [
    "#다른 날짜 데이터 추가\n",
    "df = pd.concat([df, make_total_data(1, code_list, 20200505)])\n",
    "df = pd.concat([df, make_total_data(1, code_list, 20200507)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.43      0.75      0.55         4\n",
      "          사회       0.57      1.00      0.73         4\n",
      "       생활/문화       1.00      0.14      0.25         7\n",
      "\n",
      "    accuracy                           0.53        15\n",
      "   macro avg       0.67      0.63      0.51        15\n",
      "weighted avg       0.73      0.53      0.46        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#다른 날짜 기사 포함 분석 진행\n",
    "tokenizer = Mecab()\n",
    "\n",
    "text_data = preprocessing(df['news'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터를 합치기 위해 Pandas의 concat() 함수를 이용하여 기존의 DataFrame과 새로 수집된 데이타의 DataFrame을 합칠 수 있음\n",
    "\n",
    "- 테스트 데이타 섞기는 train_test_split() 함수에서 random으로 알아서 섞어 줌..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
