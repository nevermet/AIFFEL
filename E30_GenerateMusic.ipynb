{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import concurrent.futures\n",
    "\n",
    "import mido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플로 1개의 MIDI 파일을 골라봅니다.\n",
    "midi_file = os.getenv('HOME')+'/aiffel/music_transformer/data/maestro-v2.0.0/2018/MIDI-Unprocessed_Chamber1_MID--AUDIO_07_R3_2018_wav--2.midi'\n",
    "\n",
    "midi = mido.MidiFile(midi_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG [0]----------------\n",
      "0\n",
      "set_tempo\n",
      "MSG [1]----------------\n",
      "0\n",
      "time_signature\n",
      "MSG [2]----------------\n",
      "0\n",
      "program_change\n",
      "MSG [3]----------------\n",
      "0\n",
      "control_change\n",
      "MSG [4]----------------\n",
      "0\n",
      "control_change\n",
      "MSG [5]----------------\n",
      "0.5143229166666666\n",
      "control_change\n",
      "MSG [6]----------------\n",
      "0.6328125\n",
      "control_change\n",
      "MSG [7]----------------\n",
      "0.7903645833333333\n",
      "control_change\n",
      "MSG [8]----------------\n",
      "0.9999999999999999\n",
      "control_change\n",
      "MSG [9]----------------\n",
      "1.0325520833333333\n",
      "note_on\n",
      "[1.0325520833333333, 1, 74, 86]\n",
      "MSG [10]----------------\n",
      "1.0442708333333333\n",
      "note_on\n",
      "[1.0442708333333333, 1, 38, 77]\n",
      "MSG [11]----------------\n",
      "1.0794270833333333\n",
      "control_change\n",
      "MSG [12]----------------\n",
      "1.1184895833333333\n",
      "control_change\n",
      "MSG [13]----------------\n",
      "1.1588541666666665\n",
      "control_change\n",
      "MSG [14]----------------\n",
      "1.2174479166666665\n",
      "control_change\n",
      "MSG [15]----------------\n",
      "1.2265624999999998\n",
      "note_on\n",
      "[1.2265624999999998, 0, 74, 0]\n",
      "MSG [16]----------------\n",
      "1.2369791666666665\n",
      "control_change\n",
      "MSG [17]----------------\n",
      "1.2395833333333333\n",
      "note_on\n",
      "[1.2395833333333333, 1, 73, 69]\n",
      "MSG [18]----------------\n",
      "1.2408854166666665\n",
      "note_on\n",
      "[1.2408854166666665, 1, 37, 64]\n",
      "MSG [19]----------------\n",
      "1.2460937499999998\n",
      "note_on\n",
      "[1.2460937499999998, 0, 38, 0]\n",
      "MSG [20]----------------\n",
      "1.2565104166666665\n",
      "control_change\n",
      "MSG [21]----------------\n",
      "1.2695312499999998\n",
      "note_on\n",
      "[1.2695312499999998, 1, 34, 64]\n",
      "MSG [22]----------------\n",
      "1.2734374999999998\n",
      "note_on\n",
      "[1.2734374999999998, 1, 71, 71]\n",
      "MSG [23]----------------\n",
      "1.2760416666666665\n",
      "control_change\n",
      "MSG [24]----------------\n",
      "1.2968749999999998\n",
      "control_change\n",
      "MSG [25]----------------\n",
      "1.309895833333333\n",
      "note_on\n",
      "[1.309895833333333, 0, 34, 0]\n",
      "MSG [26]----------------\n",
      "1.3164062499999998\n",
      "control_change\n",
      "MSG [27]----------------\n",
      "1.3164062499999998\n",
      "note_on\n",
      "[1.3164062499999998, 0, 73, 0]\n",
      "MSG [28]----------------\n",
      "1.3242187499999998\n",
      "note_on\n",
      "[1.3242187499999998, 1, 35, 64]\n",
      "MSG [29]----------------\n",
      "1.3242187499999998\n",
      "note_on\n",
      "[1.3242187499999998, 0, 37, 0]\n",
      "MSG [30]----------------\n",
      "1.3359374999999998\n",
      "control_change\n",
      "MSG [31]----------------\n",
      "1.3437499999999998\n",
      "note_on\n",
      "[1.3437499999999998, 0, 71, 0]\n"
     ]
    }
   ],
   "source": [
    "ON = 1\n",
    "OFF = 0\n",
    "CC = 2\n",
    "\n",
    "current_time = 0\n",
    "eventlist = []\n",
    "cc = False\n",
    "for idx, msg in enumerate(midi):\n",
    "    print('MSG [{}]----------------'.format(idx))\n",
    "    current_time += msg.time\n",
    "    print(current_time)\n",
    "    print(msg.type)\n",
    "    if msg.type is 'note_on' and msg.velocity > 0:\n",
    "        event = [current_time, ON, msg.note, msg.velocity]\n",
    "        print(event)\n",
    "    elif msg.type is 'note_off' or (msg.type is 'note_on' and msg.velocity == 0):\n",
    "        event = [current_time, OFF, msg.note, msg.velocity]\n",
    "        print(event)\n",
    "        \n",
    "    if msg.type is 'control_change':\n",
    "        if msg.control != 64:\n",
    "            continue\n",
    "        if cc == False and msg.value > 0:\n",
    "            cc = True\n",
    "            event = [current_time, CC, 0, 1]\n",
    "            print(event)\n",
    "        elif cc == True and msg.value == 0:\n",
    "            cc = False\n",
    "            event = [current_time, CC, 0, 0]\n",
    "            print(event)\n",
    "\n",
    "    if idx > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntervalDim = 100\n",
    "\n",
    "VelocityDim = 32\n",
    "VelocityOffset = IntervalDim\n",
    "\n",
    "NoteOnDim = NoteOffDim = 128\n",
    "NoteOnOffset = IntervalDim + VelocityDim\n",
    "NoteOffOffset = IntervalDim + VelocityDim + NoteOnDim\n",
    "\n",
    "CCDim = 2\n",
    "CCOffset = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim + CCDim # 390\n",
    "\n",
    "\n",
    "def get_data(data, length):    \n",
    "    # time augmentation\n",
    "    data[:, 0] *= np.random.uniform(0.80, 1.20)\n",
    "    \n",
    "    # absolute time to relative interval\n",
    "    data[1:, 0] = data[1:, 0] - data[:-1, 0]\n",
    "    data[0, 0] = 0\n",
    "    \n",
    "    # discretize interval into IntervalDim\n",
    "    data[:, 0] = np.clip(np.round(data[:, 0] * IntervalDim), 0, IntervalDim - 1)\n",
    "    \n",
    "    # Note augmentation\n",
    "    data[:, 2] += np.random.randint(-6, 6)\n",
    "    data[:, 2] = np.clip(data[:, 2], 0, NoteOnDim - 1)\n",
    "    \n",
    "    eventlist = []\n",
    "    for d in data:\n",
    "        # append interval\n",
    "        interval = d[0]\n",
    "        eventlist.append(interval)\n",
    "    \n",
    "        # note on case\n",
    "        if d[1] == 1:\n",
    "            velocity = (d[3] / 128) * VelocityDim + VelocityOffset\n",
    "            note = d[2] + NoteOnOffset\n",
    "            eventlist.append(velocity)\n",
    "            eventlist.append(note)\n",
    "            \n",
    "        # note off case\n",
    "        elif d[1] == 0:\n",
    "            note = d[2] + NoteOffOffset\n",
    "            eventlist.append(note)\n",
    "        # CC\n",
    "        elif d[1] == 2:\n",
    "            event = CCOffset + d[3]\n",
    "            eventlist.append(event)\n",
    "            \n",
    "    eventlist = np.array(eventlist).astype(np.int)\n",
    "\n",
    "    if len(eventlist) > (length+1):\n",
    "        start_index = np.random.randint(0, len(eventlist) - (length+1))\n",
    "        eventlist = eventlist[start_index:start_index+(length+1)]\n",
    "        \n",
    "    # pad zeros\n",
    "    if len(eventlist) < (length+1):\n",
    "        pad = (length+1) - len(eventlist)\n",
    "        eventlist = np.pad(eventlist, (pad, 0), 'constant')\n",
    "        \n",
    "    x = eventlist[:length]\n",
    "    y = eventlist[1:length+1]\n",
    "    \n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1282,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.getenv('HOME') + '/aiffel/music_transformer/data/midi_test.npy'\n",
    "\n",
    "get_midi = np.load(data_path, allow_pickle=True)\n",
    "get_midi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 256\n",
    "train = []\n",
    "labels = []\n",
    "\n",
    "for midi_list in get_midi:\n",
    "    cut_list = [midi_list[i:i+length] for i in range(0, len(midi_list), length)]\n",
    "    for sublist in cut_list:\n",
    "        x , y = get_data(np.array(sublist), length)\n",
    "        train.append(x)\n",
    "        labels.append(y)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59268, 256) (59268, 256)\n"
     ]
    }
   ],
   "source": [
    "train = np.array(train)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(train.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pad = pad_sequences(train,\n",
    "                               maxlen=length,\n",
    "                               padding='post',\n",
    "                               value=0)\n",
    "train_label_pad = pad_sequences(labels,\n",
    "                                maxlen=length,\n",
    "                                padding='post',\n",
    "                                value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_casting(train, label):\n",
    "    train = tf.cast(train, tf.int64)\n",
    "    label = tf.cast(label, tf.int64)\n",
    "\n",
    "    return train, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data_pad, train_label_pad))\n",
    "train_dataset = train_dataset.map(tensor_casting)\n",
    "train_dataset = train_dataset.shuffle(10000).batch(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  0   0   0 ...  35 284  99]\n",
      " [198   1 112 ... 110 198   6]\n",
      " [203   3 328 ... 320   3 117]\n",
      " ...\n",
      " [117 191   2 ... 183   5 311]\n",
      " [187  18 315 ... 218   1 112]\n",
      " [  0   0   0 ...   1 388   0]], shape=(16, 256), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[  0   0   0 ... 284  99 388]\n",
      " [  1 112 191 ... 198   6 326]\n",
      " [  3 328   0 ...   3 117 183]\n",
      " ...\n",
      " [191   2 315 ...   5 311   0]\n",
      " [ 18 315   4 ...   1 112 213]\n",
      " [  0   0   0 ... 388   0 319]], shape=(16, 256), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for t,l in train_dataset.take(1):\n",
    "    print(t)\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 1), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeGlobalAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(RelativeGlobalAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.headDim = d_model // num_heads\n",
    "        self.contextDim = int(self.headDim * self.num_heads)\n",
    "        self.eventDim = 390\n",
    "        self.E = self.add_weight('E', shape=[self.num_heads, length, self.headDim])\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(self.headDim)\n",
    "        self.wk = tf.keras.layers.Dense(self.headDim)\n",
    "        self.wv = tf.keras.layers.Dense(self.headDim)\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        q = tf.stack([self.wq(q) for _ in range(self.num_heads)])\n",
    "        k = tf.stack([self.wk(k) for _ in range(self.num_heads)])\n",
    "        v = tf.stack([self.wv(v) for _ in range(self.num_heads)])\n",
    "\n",
    "        self.batch_size = q.shape[1]\n",
    "        self.max_len = q.shape[2]\n",
    "        \n",
    "        #skewing\n",
    "        # E = Heads, Time, HeadDim\n",
    "        # [Heads, Batch * Time, HeadDim]\n",
    "        Q_ = tf.reshape(q, [self.num_heads, self.batch_size * self.max_len, self.headDim])\n",
    "        # [Heads, Batch * Time, Time]\n",
    "        S = tf.matmul(Q_, self.E, transpose_b=True)\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = tf.reshape(S, [self.num_heads, self.batch_size, self.max_len, self.max_len])\n",
    "        # [Heads, Batch, Time, Time+1]\n",
    "        S = tf.pad(S, ((0, 0), (0, 0), (0, 0), (1, 0)))\n",
    "        # [Heads, Batch, Time+1, Time]\n",
    "        S = tf.reshape(S, [self.num_heads, self.batch_size, self.max_len + 1, self.max_len])   \n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = S[:, :, 1:]\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        attention = (tf.matmul(q, k, transpose_b=True) + S) / np.sqrt(self.headDim)\n",
    "        # mask tf 2.0 == tf.linalg.band_part\n",
    "        get_mask = tf.linalg.band_part(tf.ones([self.max_len, self.max_len]), -1, 0)\n",
    "        attention = attention * get_mask - tf.cast(1e10, attention.dtype) * (1-get_mask)\n",
    "        score = tf.nn.softmax(attention, axis=3)\n",
    "\n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        context = tf.matmul(score, v)\n",
    "        # [Batch, Time, Heads, HeadDim]\n",
    "        context = tf.transpose(context, [1, 2, 0, 3])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        context = tf.reshape(context, [self.batch_size, self.max_len, self.d_model])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        logits = tf.keras.layers.Dense(self.d_model)(context)\n",
    "\n",
    "        return logits, score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.rga = RelativeGlobalAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.rga(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.rga1 = RelativeGlobalAttention(d_model, num_heads)\n",
    "        self.rga2 = RelativeGlobalAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.rga1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.rga2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attention_weights = {}\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "    def call(self, inp, training, enc_padding_mask, \n",
    "             look_ahead_mask, dec_padding_mask):\n",
    "        embed = self.embedding(inp)\n",
    "        embed *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        enc_output = self.encoder(embed, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            embed, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 390   # MIDI가 낼 수 있는 소리의 종류\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "music_transformer = MusicTransformer(num_layers, d_model, num_heads, dff,\n",
    "                                     input_vocab_size, rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.getenv('HOME')+'/aiffel/music_transformer/models/'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(music_transformer=music_transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 6.3373\n"
     ]
    }
   ],
   "source": [
    "#EPOCHS = 20  \n",
    "EPOCHS = 1  # 1epoch가 매우 오래 걸립니다. \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = music_transformer(inp, True, None, None, None)\n",
    "            loss = loss_function(tar, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, music_transformer.trainable_variables)    \n",
    "        optimizer.apply_gradients(zip(gradients, music_transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result()))\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
